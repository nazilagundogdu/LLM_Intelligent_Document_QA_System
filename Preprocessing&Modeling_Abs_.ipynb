{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modeling\n",
    "\n",
    "This notebook does the preprocessing of the data scraped from the `arXiv.org` on \"Alzheimer's Disease\". The goal of preprocessing is to prepare the data for the LLM interpretation as part of the Retrieval Augmented Generation (RAG) architecture, added below:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"img/RAG-Architecture.png\" alt=\"rag\" width=\"600\"/><br>\n",
    "  <em>Picture reference: Litvinov, A. (2024, Feb 19). How was @ZoomcampQABot made? \n",
    "  <a href=\"https://docs.google.com/presentation/d/1Z__Qo7g8j6TWxMN0yxmVeXGyji0QmA2q4zTCQt4Zgs4/edit?slide=id.p#slide=id.p\" target=\"_blank\">Google Slides presentation</a></em>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Note:** Due to limitations on computational resources, I am using a small LLM, and small data size. The point of this exercise is illustrating the RAG pipeline having access to online data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Model Response before RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "ask_llm = pipeline(\n",
    "    model= model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"At what age do people usually start showing AD symptoms?\"\n",
    "prompt_2 = \"What is the latest development in treating AD?\"\n",
    "prompt_3 = \"At what age do people usually start showing Alzheimer's Disease symptoms?\"\n",
    "prompt_4 = prompt_3 + \" Give me a number.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At what age do people usually start showing AD symptoms?\n"
     ]
    }
   ],
   "source": [
    "llm_response_1 = ask_llm(prompt_1)[0][\"generated_text\"]\n",
    "print(llm_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the latest development in treating AD?\n"
     ]
    }
   ],
   "source": [
    "llm_response_2 = ask_llm(prompt_2)[0][\"generated_text\"]\n",
    "print(llm_response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At what age do people usually start showing Alzheimer's Disease symptoms?\n"
     ]
    }
   ],
   "source": [
    "llm_response_3 = ask_llm(prompt_3)[0][\"generated_text\"]\n",
    "print(llm_response_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without RAG, the model keeps repeating the question without providing any answer. Let's try to change the prompt, giving some tips to the model. We get a slightly more informative response. But, still not quite right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At what age do people usually start showing Alzheimer's Disease symptoms? Give me a number.\n"
     ]
    }
   ],
   "source": [
    "llm_response_4 = ask_llm(prompt_4)[0][\"generated_text\"]\n",
    "print(llm_response_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this notebook is not on prompt engineering. One could argue that with more trial and error, we can find a prompt that works relatively well. But, the idea is to do with minimum prompt engineering, rather to rely on facts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "## 1.1. Setup\n",
    "The following need to be installed once. Commented out because of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ni3ktf2jSmQX",
    "outputId": "c48871eb-0599-476f-96e3-f4d3b98758c6"
   },
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama_index.embeddings.huggingface\n",
    "# !pip install llama_index\n",
    "# !pip install llama_index.llms.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yh7RV7e4ScU2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # for handling structured data, here .json\n",
    "import requests # to download files or make HTTP requests\n",
    "import nest_asyncio  # allows running async code in environments like Jupyter notebooks\n",
    "nest_asyncio.apply()  # applies the asyncio patch to enable nested event loops\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader  # for loading documents from a local directory\n",
    "from llama_index.core import Document  # used to convert raw text into Document objects for processing\n",
    "from llama_index.core.node_parser import SentenceSplitter  # for splitting documents into smaller text chunks (nodes)\n",
    "from llama_index.core import Settings  # to configure global settings like LLMs or embedding models\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # to use Hugging Face models for generating embeddings\n",
    "from llama_index.core import VectorStoreIndex  # to build a vector index for retrieval and search\n",
    "from transformers import pipeline  # provides access to Hugging Face pre-trained models for tasks like text generation or classification\n",
    "from huggingface_hub import notebook_login  # to authenticate with Hugging Face and access gated models or datasets\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM  # to use a Hugging Face language model (LLM) as the backend for generating responses in LlamaIndex\n",
    "from llama_index.core.indices.list import ListIndex # import a simple sequential index for storing and querying documents as a list\n",
    "from sentence_transformers import SentenceTransformer, util  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "1uqgLvHYSvfc",
    "outputId": "326b528d-19cc-4e09-88a3-6ee5a304a931"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>author</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2111.08794v2</td>\n",
       "      <td>2021-11-16T21:48:09Z</td>\n",
       "      <td>Investigating Conversion from Mild Cognitive I...</td>\n",
       "      <td>Alzheimer's disease is the most common cause o...</td>\n",
       "      <td>[{'name': 'Deniz Sezin Ayvaz'}, {'name': 'Inci...</td>\n",
       "      <td>Inci M. Baytas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/1411.4221v1</td>\n",
       "      <td>2014-11-16T06:39:23Z</td>\n",
       "      <td>A dynamic mechanism of Alzheimer based on arti...</td>\n",
       "      <td>In this paper, we provide another angle to ana...</td>\n",
       "      <td>[{'name': 'Zhi Cheng'}]</td>\n",
       "      <td>Zhi Cheng</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/1509.02273v2</td>\n",
       "      <td>2015-09-08T08:02:18Z</td>\n",
       "      <td>Reduction of Alzheimer's disease beta-amyloid ...</td>\n",
       "      <td>Alzheimer's disease is the most common form of...</td>\n",
       "      <td>[{'name': 'T. Harach'}, {'name': 'N. Marungrua...</td>\n",
       "      <td>T. Bolmont</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2409.05989v1</td>\n",
       "      <td>2024-09-09T18:31:39Z</td>\n",
       "      <td>A Comprehensive Comparison Between ANNs and KA...</td>\n",
       "      <td>Alzheimer's Disease is an incurable cognitive ...</td>\n",
       "      <td>[{'name': 'Akshay Sunkara'}, {'name': 'Sriram ...</td>\n",
       "      <td>Himesh Anumala</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2402.11931v1</td>\n",
       "      <td>2024-02-19T08:18:52Z</td>\n",
       "      <td>Soft-Weighted CrossEntropy Loss for Continous ...</td>\n",
       "      <td>Alzheimer's disease is a common cognitive diso...</td>\n",
       "      <td>[{'name': 'Xiaohui Zhang'}, {'name': 'Wenjie F...</td>\n",
       "      <td>Mangui Liang</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                link             published  \\\n",
       "0  http://arxiv.org/abs/2111.08794v2  2021-11-16T21:48:09Z   \n",
       "1   http://arxiv.org/abs/1411.4221v1  2014-11-16T06:39:23Z   \n",
       "2  http://arxiv.org/abs/1509.02273v2  2015-09-08T08:02:18Z   \n",
       "3  http://arxiv.org/abs/2409.05989v1  2024-09-09T18:31:39Z   \n",
       "4  http://arxiv.org/abs/2402.11931v1  2024-02-19T08:18:52Z   \n",
       "\n",
       "                                               title  \\\n",
       "0  Investigating Conversion from Mild Cognitive I...   \n",
       "1  A dynamic mechanism of Alzheimer based on arti...   \n",
       "2  Reduction of Alzheimer's disease beta-amyloid ...   \n",
       "3  A Comprehensive Comparison Between ANNs and KA...   \n",
       "4  Soft-Weighted CrossEntropy Loss for Continous ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Alzheimer's disease is the most common cause o...   \n",
       "1  In this paper, we provide another angle to ana...   \n",
       "2  Alzheimer's disease is the most common form of...   \n",
       "3  Alzheimer's Disease is an incurable cognitive ...   \n",
       "4  Alzheimer's disease is a common cognitive diso...   \n",
       "\n",
       "                                             authors          author  \\\n",
       "0  [{'name': 'Deniz Sezin Ayvaz'}, {'name': 'Inci...  Inci M. Baytas   \n",
       "1                            [{'name': 'Zhi Cheng'}]       Zhi Cheng   \n",
       "2  [{'name': 'T. Harach'}, {'name': 'N. Marungrua...      T. Bolmont   \n",
       "3  [{'name': 'Akshay Sunkara'}, {'name': 'Sriram ...  Himesh Anumala   \n",
       "4  [{'name': 'Xiaohui Zhang'}, {'name': 'Wenjie F...    Mangui Liang   \n",
       "\n",
       "  arxiv_affiliation  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data/alzheimer.json').T\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Convert to Document\n",
    "\n",
    "The most important field is the \"abstract\" or \"summary\" colunmn, which has the most information. So, I use the information in that column to feed the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(text=s) for s in df[\"summary\"].tolist()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Split to Chunks/Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C5MtIdbdTJr3"
   },
   "outputs": [],
   "source": [
    "# initiate a splitter\n",
    "splitter = SentenceSplitter(chunk_size=200,\n",
    "                           chunk_overlap=20)\n",
    "\n",
    "# create nodes from the documents using the splitter\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BW32vXrXT2YM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# initiate an embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# vectorize the data\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling\n",
    "\n",
    "In this section, we use the prepared data (vectorized data) to feed the model. \n",
    "\n",
    "## 2.1. Logging in and Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "f4938fe32db34162afd78ea2e5c0627a",
      "7e7e0ee4f94745e890f5c8a50d928e26",
      "772a59259aac45c99ede9f4e0dffbb03",
      "07b6c0c9187749b79c8663bdba5cf7c9",
      "d2f567f919e34b37a60371b9848cf252",
      "4bb55838d5ea4fd6907a732b9bca6747",
      "c34aecac62fb42eb9c2bbe567b301d41",
      "8dd7ddfa22d64d0b905cc0875edba132",
      "54b0b707615a4c7ebb4115dd7580e7f8",
      "3a4b532a2d1c478ab6c63f2f3bffc840",
      "862a13d035d546d398f4d9c9a33b062c",
      "f72b1368e8754603b2441d3311139e31",
      "874f2b51a7bf4671a322610b3789f5b5",
      "962a07d6690c4aa2bf100f54b00f3841",
      "c12e5977d5874dc1b6807a04ffa13044",
      "cb34f7d0cc864888b1e5304ea8c12bac",
      "8dbd5409c27a4ae28f98b34618b9948d",
      "a2ec9b9a2f1f4f5292c20820a28b46fd",
      "3409548e9c1541fca1aabe8c28891114",
      "b213d3393ce342199c1afd9e8969f6ea"
     ]
    },
    "id": "DKxSYEg2URhA",
    "outputId": "4e212335-bbf9-4f70-8ca3-3fbf38b05a96"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0b18e7d04748cd87a8e7981bcbbbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# login into your hugging face account. You need to create a token \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "0da0c88abe544906ad2fed332c06d023",
      "e69d5560888441debc8ea827800d2714",
      "9e68f2668e2c45e095cc886de84b8b0c",
      "e5d9f53acb6346f8b547fb48b25b94b6",
      "a7543cfabb524c10974f35c98e3d053c",
      "a3b998e4f0544d1ead9bf81ba7829b2e",
      "bc1c297b4fcf4c0d9c286f43edc82540",
      "ef919fa9acb1467397ae2be7236fd7dc",
      "ddf3c36657cf471da69ce87bb0f84d56",
      "96b9418b7ac14575bb1c81dbdbb92292",
      "efacda515742418fb66b8b7ddf2709e2",
      "36caad4c66a749899b12bca40cb28de1",
      "32e75a3b934949cdaa05c41a8607e238",
      "ee129f59002e4443a3c764103828051d",
      "ced91838b00a4d66a28742003b20acbe",
      "1d550519ebcd4f2894281a7778a0195d",
      "4884a4f7f50648f286768b34d4daec1b",
      "1ff0285f82894bf1bda218d8769ed04c",
      "556891047f4845309797c352cb2975f2",
      "494b99c03ffd4ec893763cf829195063",
      "c587dcb613684bc1a55d2b6a788d5276",
      "8703a886b65e41fba4b96e472adef8c1",
      "d02503fd2d724b8d8a737d9de62f4d92",
      "6e91da65017447c29c1a7b92b822ca8c",
      "c2b5ea1c00944a5188b6fc8c05a7966f",
      "bae20a4556f2485c8b45a330b33429d7",
      "4d5eb204cc1e41d9a07513ee9ea96de0",
      "15a3a96254d84829b00de572f894d670",
      "cc94dd17abc446c9a0eacd77073e4800",
      "e18301c7651849f4a995f72c753f0d83",
      "11ea6009d6f2469bba922886ea5bb493",
      "218d10301acc4d62851c0224f18ed1b4",
      "ea81d013f8354045b5a2b9d01a64b4f5",
      "04f2decdd9a348978b502fd137b8ed41",
      "902643d2d6ab48fb9affb939664fb4f2",
      "321b2658cc5a443e8fd7988299d265f7",
      "2f4d1ef625504912b17272939644949a",
      "c05fb0b7ce3a429a9f47438a0d69b323",
      "a2ccd68c911f4e57b837bd2049c7fecb",
      "ac479a2b19a649b094ac2d6112d9f1e3",
      "c5a4568d9ede4892bba0faa2db83e6a5",
      "f2b07ef1a4b140c99a0055d5a35ce9b0",
      "0a6671c7fcff43f6a45724531834ff90",
      "1db44cd729d84aae891a983277048979"
     ]
    },
    "id": "BQ_AN7aBboo6",
    "outputId": "a95fb6d0-ce8c-47d9-9cf9-2bbf6355abde",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# define your model with desires parameters\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # match model and tokenizer\n",
    "    device_map=\"cpu\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "\n",
    "# setting this Hugging Face model (llm) as the default LLM\n",
    "Settings.llm = llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Invoke LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 years of age is the age at which most people start showing AD symptoms.\n"
     ]
    }
   ],
   "source": [
    "RAG_response_1 = query_engine.query(prompt_1) # Reminder: prompt_1 = \"At what age do people usually start showing AD symptoms?\"\n",
    "print(RAG_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multitarget molecules, especially those targeting neuronal membrane\n",
      "protection, could offer a comprehensive approach to AD therapy, advocating for\n",
      "further research into their mechanisms and therapeutic potential.\n"
     ]
    }
   ],
   "source": [
    "RAG_response_2 = query_engine.query(prompt_2) # Reminder: prompt_2 = \"What is the latest development in treating AD?\"\n",
    "print(RAG_response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 years or older.\n"
     ]
    }
   ],
   "source": [
    "RAG_response_3 = query_engine.query(prompt_3) # Reminder: prompt_3 = \"At what age do people usually start showing Alzheimer's Disease symptoms?\"\n",
    "print(RAG_response_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 years is the age at which Alzheimer's Disease symptoms usually start showing.\n"
     ]
    }
   ],
   "source": [
    "RAG_response_4 = query_engine.query(prompt_4) # Reminder: prompt_4 = prompt_3 + \" Give me a number.\"\n",
    "print(RAG_response_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save computational resources, one can use a chunk of the data as well as the `ListIndex` package of `hugging face` as an alternative. \n",
    "\n",
    "The query with smaller set of data is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB5OPzCTT5fK",
    "outputId": "2a2f9507-cec9-4f51-c817-a5d6ca09d853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60-70 years old is the typical age range for showing AD symptoms.\n"
     ]
    }
   ],
   "source": [
    "# using part of data , as the whole data could not be analyzed given the current computational resources\n",
    "small_node_list = nodes[:5]\n",
    "\n",
    "# using ListIndex instead of vectorized index, defined above for the same reason.\n",
    "small_index = ListIndex(nodes=small_node_list)\n",
    "query_engine = small_index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(prompt_1)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The latest development in treating AD is the development of new drugs and\n",
      "therapies. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new drugs and therapies is a crucial step in\n",
      "treating AD. The development of new\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt_2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 years of age is the average age at which Alzheimer's Disease symptoms\n",
      "begin to appear.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt_3)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 years old is the average age at which Alzheimer's Disease symptoms are first noticed.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt_4)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see even the simplified version provides an acceptable answer, although not as elaborate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Having big data, like what we have here, seems to be confusing to the model. In a recent study, it is shown that \"...while [LLMs] perform well in short contexts (<1K), performance degrades significantly as context length increases ([Ref](https://arxiv.org/pdf/2502.05167v3)).\n",
    "\n",
    "One way to circumvent this issue and boost model performance is by retreiving relative chunks from the context using semantic search. This is followed by reranking of the most relevant chunks and using the one with highest score. The chart below elaborates on the architecture.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"img/Retrieval.png\" alt=\"rag\" width=\"700\"/><br>\n",
    "  <em>Picture reference: Litvinov, A. (2024, Feb 19). How was @ZoomcampQABot made? \n",
    "  <a href=\"https://docs.google.com/presentation/d/1Z__Qo7g8j6TWxMN0yxmVeXGyji0QmA2q4zTCQt4Zgs4/edit?slide=id.p#slide=id.p\" target=\"_blank\">Google Slides presentation</a></em>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "I use the \"all-MiniLM-L6-v2\" embedding model for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load embedder on CPU to avoid MPS issues\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is too big as is. And it is causing memory shortage errors. \n",
    "\n",
    "For that reason, I use the nodes I created above by chunking the text to make it smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [node.text for node in nodes[:5000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bad3e36dda94481a9b44b0cea30b4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 5000\n",
      "Embeddings shape: (5000, 384)\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings_np = embedder.encode(\n",
    "    corpus,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Corpus length:\", len(corpus))                    # Should be 5000\n",
    "print(\"Embeddings shape:\", corpus_embeddings_np.shape)  # Should be (5000, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that given the arguments returns the relevant corpus chunk together with the similarity score\n",
    "def semantic_search(query, corpus, corpus_embeddings_np, embedder, top_k=5):\n",
    "    query_embedding_np = embedder.encode(query, convert_to_numpy=True)\n",
    "    query_embedding = torch.tensor(query_embedding_np, dtype=torch.float32).cpu() #using CPU exlicitly to avoid MPS issues \n",
    "    corpus_tensor = torch.tensor(corpus_embeddings_np, dtype=torch.float32).cpu()\n",
    "\n",
    "    similarity_scores = util.cos_sim(query_embedding, corpus_tensor)[0]\n",
    "\n",
    "    k = min(top_k, len(corpus))\n",
    "    scores, indices = torch.topk(similarity_scores, k=k)\n",
    "\n",
    "    results = [(corpus[i], scores[idx].item()) for idx, i in enumerate(indices.tolist())]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function retrieveing relevant chunks with scores to use as context for running query\n",
    "def generate_answer(query):\n",
    "    retrieved = semantic_search(query, corpus, corpus_embeddings_np, embedder)\n",
    "    retrieved_docs = [text for text, score in retrieved]\n",
    "\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    response = query_engine.query(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results: [('The prevalence of\\nAD around the world is on the rise, with a predicted 152\\nmillion people to be affected by the disease in 2050 [1]. While\\nAD is incurable, the early detection of AD has been found\\nto help with managing cognitive symptoms and quality of life\\nproblems that might be related to the disorder [2]. AD is found\\nto be most prevalent in people who are 65 years and older,\\nwhich makes it hard to detect early on as diminishing cognitive\\ncapability can be otherwise attributed to old age within this\\ngroup [3]. A solution that can aid with the early detection of\\nAD for those of old age is crucial in ensuring that the highest\\nquality treatment is possible for this group of people.', 0.6020330190658569)]\n"
     ]
    }
   ],
   "source": [
    "semantic_results_1 = semantic_search(prompt_1, corpus, corpus_embeddings_np, embedder, top_k=1)\n",
    "print(\"Top results:\", semantic_results_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results: [('The prevalence of\\nAD around the world is on the rise, with a predicted 152\\nmillion people to be affected by the disease in 2050 [1]. While\\nAD is incurable, the early detection of AD has been found\\nto help with managing cognitive symptoms and quality of life\\nproblems that might be related to the disorder [2]. AD is found\\nto be most prevalent in people who are 65 years and older,\\nwhich makes it hard to detect early on as diminishing cognitive\\ncapability can be otherwise attributed to old age within this\\ngroup [3]. A solution that can aid with the early detection of\\nAD for those of old age is crucial in ensuring that the highest\\nquality treatment is possible for this group of people.', 0.5587400197982788)]\n"
     ]
    }
   ],
   "source": [
    "semantic_results_2 = semantic_search(prompt_2, corpus, corpus_embeddings_np, embedder, top_k=1)\n",
    "print(\"Top results:\", semantic_results_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results: [(\"Furthermore, Alzheimer's \\ndisease does not appear suddenly; dementia symptoms \\nemerge gradually. Memory loss is limited in the \\nbeginning stages of Alzheimer's, but people with late-\\nstage Alzheimer's frequently lose their capacity to \\ncommunicate and respond to their surroundings \\n(Alzheimer's Association, 2019). Even with advances in \\nneuroimaging techniques, physicians and doctors have \\ndifficulty \\ndiagnosing \\nAlzheimer's \\ndisease \\nstages. \\nApproximately 5.8 million Americans of all ages lived \\nwith Alzheimer's dementia in 2019. The distribution of \\nthis is given in Fig. 1. Manual diagnosis of Alzheimer's \\ndisease \\nis \\nsubjective \\nand \\ntime-consuming \\nand \\ngeriatricians are sometimes needed to determine the exact \\nstage of the disease.\", 0.7148532867431641)]\n"
     ]
    }
   ],
   "source": [
    "semantic_results_3 = semantic_search(prompt_3, corpus, corpus_embeddings_np, embedder, top_k=1)\n",
    "print(\"Top results:\", semantic_results_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
